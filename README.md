# NLP
Transformative QA: Analyzing BERT, ALBERT, RoBERTa and BERT-RoBERTa Ensembles.
This research presents a comprehensive evaluation of question answering models trained on the Stanford Question Answering Dataset (SQuAD), leveraging the transformative
power of Bidirectional Encoder Representations from Trans-
formers (BERT), A Lite BERT (ALBERT), and an innovative
ensemble approach combining BERT and RoBERTa. We metic-
ulously trained and evaluated each model, achieving remarkable
F1 scores. Initially, we explored the BERT model, renowned for
its deep contextual representations. Subsequently, we employed
ALBERT, chosen for its efficiency and compact model size
while maintaining performance comparable to BERT. Finally,
we implemented a novel ensemble model that amalgamates
BERT and RoBERTa, aiming to harness the strengths of both
models and further enhance question answering capabilities.
This paper elucidates the detailed methodology of our training
process, model architectures, data preprocessing techniques, and
evaluation metrics employed. Our findings contribute significant
insights into the development of high-performing question an-
swering systems and highlight the potential of ensemble models
in attaining state-of-the-art results. This work not only sets a
standard for future research efforts in question answering model
creation, but it also advances the field of Natural Language
Processing (NLP).    
![image](https://github.com/YashaGajula/NLP/assets/170789442/1edfef11-b668-44f0-a0cd-36462e3ef4c9)
![image](https://github.com/YashaGajula/NLP/assets/170789442/fb3cd474-b023-4eb0-8c58-7cff1c071e26)
![image](https://github.com/YashaGajula/NLP/assets/170789442/bbd33ffe-f10d-4c24-943a-7e376ff7ffc4)

